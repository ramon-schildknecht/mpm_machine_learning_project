---
title: "MPM Group Work - Group 3"
output: html_document
---

# Abstract

shortly show task & Solution















# Data Preparation
## Load Packages
```{r Load Packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(DataExplorer) # https://github.com/boxuancui/DataExplorer
library(glmnet)
library(magrittr)
library(janitor) # https://github.com/sfirke/janitor
library(plotly)
library(GGally)
library(corrplot)
library(PerformanceAnalytics)
library(caret)
library(leaps)
```


## Set Options
```{r Set Options}
# set printing preferences
options(scipen = 99) # penalty for displaying scientific notation
options(digits = 4) # suggested number of digits to display
```


## Load Data

The following code loads the data from Kaggle, filters the data to the date range from 2007 to 2015 and selects the subset for group 3 according to the lecturer instructions. One can skip the following part because our team provids the prepared data in the next section.

```{r Data Load 1/2, eval=FALSE, include=FALSE}
# preparation step: loading data from https://www.kaggle.com/wendykan/lending-club-loan-data
# load data
data_temp <- read_csv("data/loan.csv")

# select years 2007 - 2015
data_temp %<>% 
  mutate(year_temp = as.integer(str_sub(issue_d,-4,-1))) %>%
  filter(year_temp >= 2007,
         year_temp <= 2015)

# subset our group subset
# d <- data_temp[which(data_temp$id%%8+1==6),] # not working: no id values

data_temp_2 <- cbind(id_2 = rownames(data_temp), data_temp)
rownames(data_temp_2) <- 1:nrow(data_temp_2)
data_temp_2$id_2 <- as.numeric(data_temp_2$id_2)
dataset_3 <- data_temp_2[which(data_temp_2$id_2%%8+1==3),]

# we continue working with d for easier coding reasons
d <- dataset_3 

# transform data frame into much nicer tibble format for working with
# huge datasets
d <- as_tibble(d)

# remove big data file for memory reasons
rm(data_temp)
rm(data_temp_2)
rm(dataset_3)

# write group 3 data
path_file <- "./data/data_group_3.csv"
if (!file.exists(path_file)) {
  write_csv(d, path = "data/data_group_3.csv")
}
```

One can load the necessary data for the regression and classification in the following section.

```{r Data Load 2/2}
# import already generated file for group 3 
d <- read_csv("data/data_group_3.csv") #, n_max = 2000) 
d <- as_tibble(d)
```


## Data Exploration

```{r}
compare_df_cols(d) %>% arrange(d, column_name) %>% count(d) %>% arrange(desc(n))
d %>% remove_empty(c("rows", "cols")) # shows 14 empty columns
d %>% remove_constant() # shows no column with just single values
d %>% get_dupes() # there are no duplicates
# explore interest rate
d %>% tabyl(int_rate) %>% arrange(desc(n)) %>% plot_ly(x = ~n, type = "histogram")
d %>% tabyl(int_rate) %>% arrange(desc(n)) %>% plot_ly(x = ~n, type = "box")
```









# Regression (Part 1)
## Preparation

```{r}
# create copy of dataset
d_copy <- d

# data preparation --> check for NAs in int_rate and remove them if there are any
d %>%
  select(int_rate) %>%  
  summarise_all(funs(sum(is.na(.))))
# there are no NAs --> nothing to do

#further NAs investigation
## too much variables to see all NAs attributes clearly --> split variables
plot_missing(d[1:75])
plot_missing(d[76:147])
## there are a lot of variables with an NA-Quota bigger than 5%

# generate columns where NA quota is bigger or equal than 5% of rows
d_na_quoate_bigger_equals_0.05 <- d %>% summarise_all(funs(sum(is.na(.)))) %>% t() %>% 
  as.data.frame() %>% rownames_to_column("column_names") %>% as_tibble() %>% 
  select(count = V1, everything()) %>% arrange(desc(count)) %>% 
  mutate(quote = count/length(d$id)) %>% filter(quote >= 0.05)
(d_count_columns_na_quoate_bigger_equals_0.05 <- length(d_na_quoate_bigger_equals_0.05$column_names))
# 93 columns with NA quote >= 5%

# remove 93 columns with NA quota >= 0.05%
d_filter_temp <- d %>% summarise_all(funs(sum(is.na(.)))) %>% t() %>% 
  as.data.frame() %>% rownames_to_column("column_names") %>% as_tibble() %>% 
  select(count = V1, everything()) %>% arrange(desc(count)) %>% 
  mutate(quote = count/length(d$id)) %>% filter(quote < 0.05)

d_filter_temp %<>% select(column_names)
d %<>% select(d_filter_temp$column_names)
dim(d)[2]
# success: only 54 columns left!
```

## Prepare valid dataset for regression and classification

```{r}
# inspect remaining variables
plot_missing(d) # success!

# create backup copy containing NAs
d_with_nas <- d

# remove rows with NAs
d %<>% na.omit() # 2347 NAs removed

# create new data frame for part 2 (classification)
## d_na_less_5_percent <- d
d_clean <- d
```


## Prepare Dataset for regression task

```{r}
introduce(d) %>%  t()
plot_intro(d)
plot_bar(d) # 7 columns ignored with more than 50 categories.
# last_pymnt_d: 129 categories
# title: 9652 categories
# earliest_cr_line: 624 categories
# last_credit_pull_d: 127 categories
# issue_d: 99 categories
# zip_code: 874 categories
# addr_state: 51 categories

# generate a list of unique counts per column
(d_unique_counts_per_column <- d %>% summarise_all(list(~n_distinct(.))) %>% t() %>% as.data.frame() %>% rownames_to_column("column_names") %>% as_tibble() %>% 
    select(count = V1, everything()) %>% arrange(desc(count)))

# assumption: select most important columns according to subjective evaluation
## filter e. g. columns with just one unique value, not understandable meaning, 
## in general columns <= 10 distinct values, 
d %<>% select(int_rate, loan_amnt, revol_bal, revol_util, funded_amnt_inv, annual_inc, pub_rec_bankruptcies, 
            term, pymnt_plan, initial_list_status, application_type, hardship_flag, debt_settlement_flag, 
            verification_status, home_ownership, acc_now_delinq, grade, collections_12_mths_ex_med, loan_status,
            chargeoff_within_12_mths, year_temp)

# write file for easier regression analysis start
path_file <- "./data/data_group_3_regression.csv"
if (!file.exists(path_file)) {
  write_csv(d, path = "data/data_group_3_regression.csv")
}
```


## Load prepared data
```{r}
# starting with the following data for regression task
d <- read_csv("data/data_group_3_regression.csv")
```

## Regression data exploration
```{r}
ggpairs(d[, 1:3]) 
# ggpairs(d) # not really working for all data

# show scatterplots, distributions & correlations
set.seed(22)
d %>%
  sample_n(1000) %>% 
  select(c(1:7, 16, 18, 20, 21)) %>% 
  na.omit() %>% 
  chart.Correlation(histogram = TRUE, pch = 19) # better than cor() %>%corrplot()
  

### show patterns ###
plot_bar(d, with = "int_rate")
plot_boxplot(d, by = "int_rate")
plot_histogram(d)
plot_scatterplot(split_columns(d)$continuous, by = "int_rate", sampled_rows = 1000L)
```


## Validation set approach

```{r}
# Split the data into training and test set
set.seed(22)
training_samples <- d$int_rate %>%
  createDataPartition(p = 0.67, list = FALSE)
train_data  <- d[training_samples, ]
test_data <- d[-training_samples, ]
test_data %<>% filter(home_ownership != "NONE")
```


## Models
### Exploratory linear regression
```{r}
# Build the model first try
lm_model <- lm(int_rate~., data = train_data)
summary(lm_model)

# remove not significant predictors
train_data_lm <- train_data %>% select(-pymnt_plan, -collections_12_mths_ex_med)
test_data_lm <- test_data %>% select(-pymnt_plan, -collections_12_mths_ex_med)

# second try
lm_model <- lm(int_rate~., data = train_data_lm)
summary(lm_model)

table(d$pymnt_plan) # clear distribution > most of the values are no, only 9 values are yes
table(d$collections_12_mths_ex_med) # more or less clear distribution > most of the values are in categorie 0

# other not significant predictors in the lm-Modell can't be removed because they have categories which don't
# show a clearly recognisable distribution of the values, for example
table(d$loan_status) # not a clear distribution > predictor can't be removed
table(d$home_ownership)

# Make predictions and compute the R2, RMSE and MAE
lm_predictions <- lm_model %>% predict(test_data)
summary(lm_predictions)
na_filter <- lm_predictions[!is.na(lm_predictions)]
# show measures for model quality
(lm_evaluation_vsa <-
  data.frame(
    R2 = R2(lm_predictions[na_filter], test_data$int_rate[na_filter]),
    RMSE = RMSE(lm_predictions[na_filter], test_data$int_rate[na_filter]) # ,
    # MAE = MAE(lm_predictions[na_filter], test_data$int_rate[na_filter])
  ))


# Leave one out cross validation - LOOCV
# Define training control
train_control <- trainControl(method = "LOOCV")
# Train the model
set.seed(22)
d_sample <- d %>% sample_n(2000) %>% na.omit()
# check if there is just one unique value per column: hit with "pymnt_plan" and "hardship_flag"
d_sample %>% summarise_all(list(~n_distinct(.))) %>% t() %>% as.data.frame() %>% rownames_to_column("column_names") %>% as_tibble() %>% 
  select(count = V1, everything()) %>% arrange(count)
d_sample %<>% select(-c(pymnt_plan, hardship_flag))
lm_model_loocv <- train(int_rate ~ ., data = d_sample, method = "lm",
               trControl = train_control)
# Summarize the results
(lm_evaluation_loocv <- lm_model_loocv)
print("MSE: ")
(loocv_mse <- lm_evaluation_loocv$results$RMSE^2)
# comparison to validation set approach: slightly worse at R2 and RMSE but better MAE

```




### Linear regression with best subset selection

```{r Linear regressionw with best subset selection}
dim(train_data) # 20 predictors (21-1). -1 because of int_rate

# creates a model matrix by expanding factors to a set of dummy variables 
test_mat <- model.matrix(int_rate~.,data=test_data)

# when considering all dummy variables there are 35 predictors (36-1): -1 because of intercept.
dim(test_mat)

# create best subset selection with 1 up to all 35 predictors.
regfit_best <- regsubsets(int_rate~.,data=train_data, nvmax=35)
reg_summary <- summary(regfit_best)
reg_summary # models with up to 33 predictors were included.

# Analysis of various metrics to assess the models
par(mfrow=c(2,2))
# plot R square
plot(reg_summary$rss ,xlab=" Number of Variables ",ylab="RSS", type="l")
# plot adusted R-square
plot(reg_summary$adjr2 ,xlab =" Number of Variables ", ylab="Adjusted RSq",type="l")
which.max (reg_summary$adjr2)  # maximal adusted R-square: 29
points (29, reg_summary$adjr2[29], col ="red",cex =2, pch =20)
# plot CP
plot(reg_summary$cp ,xlab =" Number of Variables ", ylab="CP",type="l")
which.min (reg_summary$cp ) # minimal CP with 26 predictors
points (26, reg_summary$cp[26], col ="red",cex =2, pch =20)
# plot BIC
plot(reg_summary$bic ,xlab =" Number of Variables ", ylab="BIC",type="l")
which.min (reg_summary$bic ) # minimal BIC with 22 predictors
points (21, reg_summary$bic[21], col ="red",cex =2, pch =20)

# model of does not seem to improve much anymore after 6 predictors are included.
coefi=coef(regfit_best ,id=6) # get coefficients
pred_6=test_mat[,names(coefi)]%*%coefi # predict value for each observation with model and test data
test_error_6=mean((test_data$int_rate-pred_6)^2) # calculate test error

# next step: compare the test MSE of the various models with 1 predictor up to 33 predictors.

test_errors=rep(NA ,33) # create empty list
# search for each size i, the best model of that size
for(i in 1:33){
  coefi=coef(regfit_best ,id=i) # get coefficients
  pred=test_mat[,names(coefi)]%*%coefi # predict value for each observation with model and test data
  test_errors[i]= mean((test_data$int_rate-pred)^2) # calculate test error
}
test_errors # show all test errors
test_errors[which.min(test_errors)] # test error of best model: 1.3799
which.min(test_errors) # model with minimal test error: 33 -> Model with all 33 variables.
par(mfrow=c(1,1))
plot(test_errors, type="l", main="Test Error") # plot the test error of all modelss
points(33, test_errors[33], col ="red",cex =2, pch =20) # mark test error of model with 33 variables.

# Plausibility Check:
# the output of best subset selection (regsubset) was that the test error decreases with every new added predictor.
# Therefore test the linear regression by including all parameters and compare the test errors:
lm_2 <- lm(int_rate~., data = train_data) # linear regression with all predictors
summary(lm_2)
lm_2_pred <- predict(lm_2, test_data) # predict int_rate based on test data
test_errors_2 <- mean((test_data$int_rate-lm_2_pred)^2) # calculate test error
test_errors_2 # test error: 1.3799
# test error is equal to previous test error that was found with best subset selection
```



### Ridge regression

```{r Ridge Regression}
# using method from http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/
# creating matrix
x=model.matrix(int_rate~., d)[, -1]
y=d$int_rate

# Predictor variables train data
x_train <- model.matrix(int_rate~., train_data)[,-1]

# Outcome variable train data
y_train <- train_data$int_rate


# Find the best lambda using cross-validation
set.seed(22) 
cv <- cv.glmnet(x_train, y_train, alpha = 0)

# Display the best lambda value using cross-validation
cv$lambda.min


# Fit the final model on the training data
model_train <- glmnet(x_train, y_train, alpha = 0, lambda = cv$lambda.min)
model_train


# Display regression coefficients
coef(model_train)


# Make predictions on the test data
x_test <- model.matrix(int_rate~., test_data)[,-1]
y_test <- test_data$int_rate

model_test <- model.matrix(int_rate ~., test_data)[,-1]
predictions <- model_train %>% predict(model_test) %>% as.vector()


# Model performance metrics
data.frame(
  RMSE = RMSE(predictions, test_data$int_rate),
  Rsquare = R2(predictions, test_data$int_rate)
)


# Mean squared error
mean((predictions - y_test)^2)


# Refit ridge regression model on the full data set, using lambda chosen by cross-validation
modell_full_data_set <- glmnet(x, y, alpha = 0)
predict(modell_full_data_set, type="coefficients", s=cv$lambda.min)
```


#### Lasso regression with 10-fold cross validation

```{r Lasso regression}
# using method from http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/
# Setup a grid range of lambda values:
lambda <- 10^seq(-3, 3, length = 100)


# Build the model
set.seed(22)
lasso <- train(
  int_rate ~., data = train_data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = cv$lambda)
)

# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
# Make predictions
predictions <- lasso %>% predict(test_data)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test_data$int_rate),
  Rsquare = R2(predictions, test_data$int_rate)
)

print("MSE")
mean((predictions-test_data$int_rate)^2)
```


## Model comparison and evaluation
Metrik: MSE nachfolgend auflisten


Best möglicher Ansatz erläutern


Nächste Schritte zur möglichen Verbesserung aufführen












# Classification (Part 2)

## Load prepared data
```{r}
d2 <- d_clean
```




